# Migrate for Anthos Pipeline Documentation

## Installing the Pipeline

1. Install `kubectl` on your workstation (matching the Kubernetes version of your cluster) following [this documentation](https://kubernetes.io/docs/tasks/tools/#kubectl). Ensure that you have authenticated `kubectl` to the cluster where you will be running the pipeline. It is possible to run the pipeline on the Migrate for Anthos processing cluster.

1. Install Tekton on your cluster following [the documentation](https://tekton.dev/docs/getting-started/#installation). Only the pipeline install is required, Triggers and Dashboard are optional. It is beneficial to install the `tkn` CLI to your workstation following [this documentation](https://tekton.dev/docs/getting-started/#set-up-the-cli) to monitor progress.

1. Setup authentication for the input source Git repository for the pipeline following [this documentation](https://tekton.dev/docs/pipelines/auth/#configuring-authentication-for-git). The created secret must be called `git-auth`. Here is an example secret YAML manifest to apply to the cluster:

   ```yaml
   apiVersion: v1
   kind: Secret
   metadata:
     name: git-auth
     annotations:
       tekton.dev/git-0: github.com # Replace with hostname of Git repository
   type: kubernetes.io/ssh-auth
   stringData:
     ssh-privatekey: |
       <private-key>
   ```

1. Build the required image for the pipeline using the `Dockerfile` in this repo and push it to a registry that will be accessible from the Kubernetes cluster running the pipeline. You can build the docker image using the command:
``` shell
docker build -t gcr.io/<<MY-GCP-PROJECT>>/python-kubectl .
```
and then push it to your container registry by running:
``` shell
docker push gcr.io/<<MY-GCP-PROJECT>>/python-kubectl:latest
```

1. Setup authentication for the registry if required. This secret must be called `reg-auth`, and can be created with this command:

   ```shell
   kubectl create secret docker-registry reg-auth \
      --docker-server= \ # <- Add registry host name
      --docker-username= \ # <- Add username
      --docker-password= \ # <- Add password/token
      --docker-email= # <- Add email
   ```

   If using gcr.io as the registry, you can use a service account for authentication by using a JSON key file with this command:

   ```shell
   kubectl create secret docker-registry reg-auth \
      --docker-server=gcr.io \
      --docker-username=_json_key \
      --docker-password="$(cat /path/to/sa-key.json)" \
      --docker-email= # <- Add service account email here
   ```

1. Install `yq` version 4.3.1 or newer on your workstation following [this documentation](https://mikefarah.gitbook.io/yq/#install). This is required by the installation script.

1. Run the installation script `install-pipeline.sh` in this repository.

## Configuring Inputs

These input files are pulled by the orchestration pipeline. Currently, Git is the only supported source.

The inputs consist of:

* A CSV file that identifies the instances to be migrated with Migrate for Anthos.
* Zero or more Migration Plan patch files, as referenced by the CSV file and used to customize the Migration Plan.

### CSV Fields
Name,VM ID,Source,AppType,DataConfig,Template
| Field | Required | Description |
|:---:|:---:|---|
| name | Yes | The name used to identify the instance being migrated. Used for the image name and application name. |
| vmId | Yes | The identifier for the instance being migrated, this identifier depends on the source location. See [documentation](https://cloud.google.com/migrate/anthos/docs/creating-a-migration#before_you_begin). |
| source | Yes | The migration source for the instance being migrated. This must be pre-created before running the pipeline. See [documentation](https://cloud.google.com/migrate/anthos/docs/adding-a-migration-source). |
| appType | Yes | The application type to be migrated. Currently, the pipeline supports `linux-system-container, windows-iis-container, tomcat-container and websphere-container`. |
| dataConfig | No | The DataConfig yaml for the migration. A sample config file can be found in [mysql-data-config.yaml](../input/mysql-data-config.yaml). |
| planPatchFile | No | The file to use for patching the Migration Plan. If not included the instance will be migrated using the default plan generated by Migrate for Anthos. These should be relative to the `planPatchPath` parameter of the pipeline. |

### Migration Plan Patch File

There are two formats supported for patching the Migration Plan:

1. **YAML Overlay**: This allows for overlaying content onto the generated Migration Plan. For data fields, this means replacing the generated value with the one in the overlay. For array fields, this means adding items to the existing array. The limitation of this method is that fields cannot be removed. An example is found at `input/sample.yaml`
1. **JSONPatch**: This allows for [RFC 6902](https://datatracker.ietf.org/doc/html/rfc6902)-formatted jsonpatch files that can modify the generated plan. This allows for full control of the plan using all the available operations. An example can be found at `input/sample.json`

The intent for these Migration Plan patch files is for them to be used for multiple similar instances to maximize reuse. To make this possible, the migration pipeline by default will change the fields `spec.image.base`, `spec.image.name`, and `spec.deployment.appName` to use the `name` field from the CSV so no instance identifying customization is necessary in the Migration Plan patch files.

## Running the pipeline

With the inputs committed and pushed to the source Git repository, the pipeline can be triggered using a PipelineRun resource with the correct parameters configured:

| Field | Required | Default | Description |
|:---:|:---:|:---:|---|
| gitURL | Yes | | This is the URL for the Git repository. |
| gitRevision | Yes | | This is the revision reference to be pulled by the pipeline. Can be a branch, a tag, or a commit hash. |
| cvsFile | Yes | input/vm-list.csv | This is the location in the Git repository of the CSV file to be parsed. |
| planPatchPath | Yes | input | This is the directory in the Git repository where the Migration Plan Patch Files can be found. |
| image | Yes | | This is the image reference as-built and pushed during installation from the `Dockerfile` in the repository |

The pipeline run should use the pre-created service account from installation, and supply a workspace for the pipeline to work in.

```yaml
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: m4a-run
spec:
  serviceAccountName: m4a-service-account
  pipelineRef:
    name: m4a-orchestration
  params:
    - name: gitURL
      value: # Input Git repository URL
    - name: gitRevision
      value: # Input Git repository branch/tag/commit hash
    - name: cvsFile
      value: # CSV file location in Git repository
    - name: planPatchPath
      value: # Migration Plan Patch Files base directory
    - name: image
      value: # Image location
  workspaces:
    - name: source
      volumeClaimTemplate:
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 256Mi
```

Apply the PipelineRun manifest to the cluster.

## Monitoring Pipeline Progress

The command `tkn pr list` will output the orchestration pipeline run under the name of the PipelineRun resource.
Additionally, it will output one migration pipeline for each line in the CSV file.

For example, if the CSV file contains:

```csv
name,vmId,source,os,intent,planPatchFile
frontend,instance-1,company-gcp,Linux,Image
backend,instance-2,company-gcp,Linux,ImageAndData,
```

Then we should eventually see the following output from `tkn pr list`

```shell
# tkn pr list
NAME               STARTED      DURATION     STATUS
frontend           1 week ago   10 minutes   Succeeded
backend            1 week ago   9 minutes    Succeeded
m4a-run            1 week ago   11 minutes   Succeeded
```

If one of the pipelines fails, then the detailed logs can be viewed with `tkn pr logs {pipeline-run-name}`. The error details will be found in the migration pipeline logs, while the orchestration logs can contain pre-validation errors and monitoring status.
